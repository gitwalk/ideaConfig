<application>
  <component name="AppStorage">
    <histories>
      <item value="Enumerate numLevels of ancestors by putting them in the stack and dispatch * the current node. * * @param nd current operator in the ancestor tree * @param level how many level of ancestors included in the stack * @param stack operator stack * @throws SemanticException" />
      <item value="walk" />
      <item value="// We did not create the table before moving the data files for a non-partitioned table i.e // we used load file instead of load table (see SemanticAnalyzer#getFileSinkPlan() for // more details). Thus could not add a write notification required for a transactional // table. Do that here, after we have created the table. Since this is a newly created // table, listing all the files in the directory and listing only the ones corresponding to // the given id doesn't have much difference." />
      <item value="Any changes you make to this file will be ignored by Hive" />
      <item value="This file is auto generated for documentation purposes ONLY" />
      <item value="Comma-separated list of post-execution hooks to be invoked for each statement. A post-execution hook is specified as the name of a Java class which implements the org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext interface." />
      <item value="Internal" />
      <item value="Since we're reusing the compiled plan, we need to update its start time for current run" />
      <item value="the reason that we set the txn manager for the cxt here is because each // query has its own ctx object. The txn mgr is shared across the // same instance of Driver, which can run multiple queries." />
      <item value="use the specified database if specified" />
      <item value="Acquires in exclusive mode, ignoring interrupts. Implemented * by invoking at least once {@link #tryAcquire}, * returning on success. Otherwise the thread is queued, possibly * repeatedly blocking and unblocking, invoking {@link * #tryAcquire} until success. This method can be used * to implement method {@link Lock#lock}." />
      <item value="acquire Queued" />
      <item value="Eagerly cache singletons to be able to resolve circular references" />
      <item value="This class contains the lineage context that is passed * while walking the operator tree in Lineage. The context * contains the LineageInfo structure that is passed to the * pre-execution hooks." />
      <item value="Indicates that the column is derived from the output * of a user script through a TRANSFORM, MAP or REDUCE syntax * or from the output of a PTF chain execution." />
      <item value="Indicates that the column is derived from a UDF, UDAF, UDTF or * set operations like union on columns on other tables * e.g. T2.c1 = T1.c1 + T3.c1." />
      <item value="Indicates that the column is derived from another table column * with no transformations e.g. T2.c1 = T1.c1." />
      <item value="Gets the new dependency type by comparing the old dependency type and the * current dependency type. The current dependency type is the dependency imposed * by the current expression. Typically the dependency type is computed using * the following rules: * SCRIPT - In case anywhere in the lineage tree there was a script operator, otherwise * EXPRESSION - In case anywhere in the lineage tree a union, * udf, udaf or udtf was done, otherwise * SIMPLE - This captures direct column copies." />
      <item value="The parse context that is used to get table metadata information" />
      <item value="A map from a final select operator id to the select operator * and the corresponding target table in case an insert into query." />
      <item value="A map from operator to the conditions strings." />
      <item value="Put the dependency in the map" />
      <item value="Processor for TableScan Operator. This actually creates the base column mappings." />
      <item value="The operator whose dependency is being inserted." />
      <item value="Puts the dependency for an operator, columninfo tuple." />
      <item value="Processor for Script and UDTF Operators." />
      <item value="list of map join operators with no reducer" />
      <item value="list of destination files being loaded" />
      <item value="list of destination tables being loaded" />
      <item value="map from table scan operator to partition pruner" />
      <item value="Make sure the basic query properties are initialized" />
      <item value="Parse Context: The current parse context. This is passed to the optimizer * which then transforms the operator tree using the parse context. All the * optimizations are performed sequentially and then the new parse context * populated. Note that since the parse context contains the operator tree, it * can be easily retrieved by the next optimization step or finally for task * generation after the plan has been completely optimized." />
      <item value="Analyze the rewritten statement" />
      <item value="So, when expanding the definition of v while analyzing the top-level query, * we tag each ASTNode with a reference to an ASTNodeOrign describing v and its * usage within the query." />
      <item value="init Parse Ctx" />
      <item value="Create a clone of the parse context" />
      <item value="Take all the driver run hooks and post-execute them." />
      <item value="// for canceling the query (should be bound to session?)" />
      <item value="/ the reason that we set the txn manager for the cxt here is because each // query has its own ctx object. The txn mgr is shared across the // same instance of Driver, which can run multiple queries." />
      <item value="compile internal will automatically reset the perf logger" />
      <item value="// the reason that we set the txn manager for the cxt here is because each // query has its own ctx object. The txn mgr is shared across the // same instance of Driver, which can run multiple queries." />
      <item value="Snapshot was outdated when locks were acquired, hence regenerate context, // txn list and retry // TODO: Lock acquisition should be moved before analyze, this is a bit hackish. // Currently, we acquire a snapshot, we compile the query wrt that snapshot, // and then, we acquire locks. If snapshot is still valid, we continue as usual. // But if snapshot is not valid, we recompile the query." />
      <item value="/*Some HiveExceptions (e.g. SemanticException) don't set canonical ErrorMsg explicitly, but there is logic (e.g. #compile()) to find an appropriate canonical error and return its code as error code. In this case we want to preserve it for downstream code to interpret*/" />
      <item value="/*Here we want to encode the error in machine readable way (e.g. JSON) * Ideally, errorCode would always be set to a canonical error defined in ErrorMsg. * In practice that is rarely the case, so the messy logic below tries to tease * out canonical error code if it can. Exclude stack trace from output when * the error is a specific/expected one. * It's written to stdout for backward compatibility (WebHCat consumes it).*/" />
      <item value="ExecuteWithHookContext is a new interface that the Pre/Post Execute Hook can run with the HookContext" />
      <item value="Based on the plan outputs, find out the target table name and column names" />
      <item value="Subclasses" />
      <item value="classLoader the class loader to create the proxy with" />
      <item value="Create a new proxy according to the settings in this factory. * &lt;p&gt;Can be called repeatedly. Effect will vary if we've added * or removed interfaces. Can add and remove interceptors. * &lt;p&gt;Uses the given class loader (if necessary for proxy creation)." />
      <item value="Uses the given class loader (if necessary for proxy creation)" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="195" />
        <entry key="ENGLISH" value="196" />
      </map>
    </option>
  </component>
  <component name="Settings">
    <option name="ignoreRegExp" value="" />
  </component>
</application>